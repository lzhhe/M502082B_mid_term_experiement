# main.py
import os
import csv
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt  # å¯¼å…¥matplotlibåº“ï¼Œç”¨äºç»˜åˆ¶å›¾è¡¨

from src.transformer_model import TransformerModel
from src.dataset import CNNDailyMailDataset
from src.bpe_tokenization import load_tokenizer

# ======================================================
# âš™ï¸ è®¾å¤‡é€‰æ‹©
# ======================================================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼ˆè·¨å¹³å°è·¯å¾„æ ¹ç›®å½•ï¼‰
BASE_DIR = os.path.dirname(os.path.abspath(__file__))


# ======================================================
# ğŸ”§ Mask å·¥å…·å‡½æ•°
# ======================================================
def generate_square_subsequent_mask(sz):
    mask = torch.triu(torch.ones(sz, sz), diagonal=1)
    mask = mask.masked_fill(mask == 1, float('-inf'))
    return mask


# ======================================================
# ğŸ‹ï¸ è®­ç»ƒå‡½æ•°
# ======================================================
def train_epoch(model, dataloader, optimizer, criterion, tokenizer, clip=1.0):
    model.train()
    total_loss = 0

    for batch in tqdm(dataloader, desc="Training"):
        src, tgt_inp, tgt_out = batch["article"], batch["decoder_input"], batch["decoder_output"]
        src, tgt_inp, tgt_out = src.to(DEVICE), tgt_inp.to(DEVICE), tgt_out.to(DEVICE)

        tgt_mask = generate_square_subsequent_mask(tgt_inp.size(1)).to(DEVICE)

        optimizer.zero_grad()
        output = model(src, tgt_inp, tgt_mask=tgt_mask)

        output_dim = output.shape[-1]
        loss = criterion(output.view(-1, output_dim), tgt_out.view(-1))
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()
        total_loss += loss.item()

    return total_loss / len(dataloader)


# ======================================================
# ğŸ§ª éªŒè¯å‡½æ•°
# ======================================================
def evaluate(model, dataloader, criterion):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            src, tgt_inp, tgt_out = batch["article"], batch["decoder_input"], batch["decoder_output"]
            src, tgt_inp, tgt_out = src.to(DEVICE), tgt_inp.to(DEVICE), tgt_out.to(DEVICE)
            tgt_mask = generate_square_subsequent_mask(tgt_inp.size(1)).to(DEVICE)

            output = model(src, tgt_inp, tgt_mask=tgt_mask)
            output_dim = output.shape[-1]
            loss = criterion(output.view(-1, output_dim), tgt_out.view(-1))
            total_loss += loss.item()
    return total_loss / len(dataloader)


# ======================================================
# ğŸ§  æ¨ç†ï¼ˆç”Ÿæˆæ‘˜è¦ï¼‰
# ======================================================
def generate_summary(model, tokenizer, src_text, max_len=80, top_k=50, top_p=0.9, temperature=1.0):
    """
    ç”Ÿæˆæ–‡æœ¬æ‘˜è¦ï¼Œä½¿ç”¨ Top-k Sampling æˆ– Nucleus Sampling
    """
    model.eval()
    src_ids = torch.tensor([tokenizer.encode(src_text, out_type=int)], dtype=torch.long).to(DEVICE)
    tgt_ids = torch.tensor([[tokenizer.bos_id()]], dtype=torch.long).to(DEVICE)

    with torch.no_grad():
        for _ in range(max_len):
            tgt_mask = generate_square_subsequent_mask(tgt_ids.size(1)).to(DEVICE)
            output = model(src_ids, tgt_ids, tgt_mask=tgt_mask)

            # è·å–ä¸‹ä¸€ä¸ª token çš„ logits
            next_token_logits = output[:, -1, :] / temperature  # ä½¿ç”¨æ¸©åº¦æ§åˆ¶åˆ†å¸ƒ

            # åº”ç”¨ Top-k Sampling
            if top_k > 0:
                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)
                top_k_probs = F.softmax(top_k_logits, dim=-1)
                next_token = torch.multinomial(top_k_probs, 1).squeeze(0)
            # åº”ç”¨ Nucleus Sampling (Top-p Sampling)
            else:
                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_logits[sorted_indices_to_remove] = -float("Inf")
                next_token = torch.argmax(F.softmax(sorted_logits, dim=-1), dim=-1)

            # å°†é‡‡æ ·çš„ token è¿½åŠ åˆ° tgt_ids
            tgt_ids = torch.cat([tgt_ids, next_token.unsqueeze(0)], dim=1)

            # å¦‚æœç”Ÿæˆçš„ token æ˜¯ EOSï¼Œåˆ™æå‰ç»“æŸ
            if next_token.item() == tokenizer.eos_id():
                break

    # è§£ç ç”Ÿæˆçš„ token ID ä¸ºæ–‡æœ¬
    decoded = tokenizer.decode(tgt_ids[0].tolist())
    return decoded


# ======================================================
# ğŸš€ ä¸»ç¨‹åºå…¥å£
# ======================================================
def main():
    print(f"ğŸš€ Using device: {DEVICE}")

    # ==== åŠ è½½ tokenizer ====
    tokenizer_path = os.path.join(BASE_DIR, "bpe_tokenizer.model")
    tokenizer = load_tokenizer(tokenizer_path)
    vocab_size = tokenizer.get_piece_size()
    print(f"âœ… Tokenizer loaded ({vocab_size} vocab size)")

    # ==== åŠ è½½æ•°æ®é›† ====
    train_data_path = os.path.join(BASE_DIR, "train_sample_filtered.parquet")
    val_data_path = os.path.join(BASE_DIR, "validation_sample_filtered.parquet")

    if not os.path.exists(train_data_path):
        raise FileNotFoundError(f"âŒ Missing training file: {train_data_path}")
    if not os.path.exists(val_data_path):
        raise FileNotFoundError(f"âŒ Missing validation file: {val_data_path}")

    train_dataset = CNNDailyMailDataset(train_data_path, tokenizer_path)
    val_dataset = CNNDailyMailDataset(val_data_path, tokenizer_path)

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=train_dataset.collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=val_dataset.collate_fn)

    # ==== åˆå§‹åŒ–æ¨¡å‹ ====
    model = TransformerModel(
        vocab_size=vocab_size,
        d_model=128,
        num_heads=2,
        num_layers=2,
        d_ff=512
    ).to(DEVICE)

    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id())
    optimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)

    # ==== è®­ç»ƒå¾ªç¯ ====
    num_epochs = 50
    best_val_loss = float('inf')
    model_save_path = os.path.join(BASE_DIR, "best_transformer.pth")

    # ç”¨äºå­˜å‚¨æ¯ä¸ªepochçš„æŸå¤±
    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        train_loss = train_epoch(model, train_loader, optimizer, criterion, tokenizer)
        val_loss = evaluate(model, val_loader, criterion)

        # ä¿å­˜æ¯ä¸ª epoch çš„æŸå¤±
        train_losses.append(train_loss)
        val_losses.append(val_loss)

        print(f"ğŸ“… Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}")

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), model_save_path)
            print(f"ğŸ’¾ Model saved at {model_save_path}")

    # ==== ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŸå¤±å›¾è¡¨ ====
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss', color='blue')
    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss', color='red')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()

    # ä¿å­˜å›¾è¡¨
    chart_save_path = os.path.join(BASE_DIR, "loss_plot.png") 
    plt.savefig(chart_save_path)
    print(f"ğŸ’¾ Loss plot saved at {chart_save_path}")

    def save_losses_to_csv(train_losses, val_losses, file_path):
        # æ‰“å¼€æ–‡ä»¶ä»¥å†™å…¥
        with open(file_path, mode='w', newline='') as file:
            writer = csv.writer(file)
            
            # å†™å…¥æ ‡é¢˜è¡Œ
            writer.writerow(["Epoch", "Train Loss", "Validation Loss"])

            # å†™å…¥æ¯ä¸ª epoch çš„æŸå¤±
            for epoch, (train_loss, val_loss) in enumerate(zip(train_losses, val_losses), start=1):
                writer.writerow([epoch, train_loss, val_loss])

        print(f"ğŸ’¾ Losses saved to {file_path}")

    # åœ¨è®­ç»ƒå®Œæˆåä¿å­˜æŸå¤±
    losses_file_path = os.path.join(BASE_DIR, "losses.csv")  # æŒ‡å®š CSV æ–‡ä»¶çš„ä¿å­˜è·¯å¾„
    save_losses_to_csv(train_losses, val_losses, losses_file_path)

    # ==== æ¨ç†æµ‹è¯• ====
    test_article = "Police and FBI agents are investigating the discovery of an empty rocket launcher tube on the front lawn of a Jersey City, New Jersey, home, FBI spokesman Sean Quinn said. Niranjan Desai discovered the 20-year-old AT4 anti-tank rocket launcher tube, a one-time-use device, lying on her lawn Friday morning, police said.The launcher has been turned over to U.S. Army officials at the 754th Ordnance Company, an explosive ordnance disposal unit, at Fort Monmouth, New Jersey, Army officials said.The launcher is no longer operable and not considered to be a hazard to public safety, police said, adding there was no indication the launcher had been fired recently.Army officials said they could not determine if the launcher had been fired, but indicated they should know once they find out where it came from. The nearest military base, Fort Dix, is more than 70 miles from Jersey City.The Joint Terrorism Task Force division of the FBI and Jersey City police are investigating the origin of the rocket launcher and the circumstance that led to its appearance on residential property.Al Qaeda doesn't leave a rocket launcher on the lawn of middle-aged ladies, said Paul Cruickshank of New York University Law School's Center on Law and Security.A neighbor, Joe Quinn, said the object lying on Desai's lawn looked military, was brown, had a handle and strap, and both ends were open, like you could shoot something with it. Quinn also said the device had a picture of a soldier on it and was 3 to 4 feet long.An Army official said the device is basically a shoulder-fired, direct-fire weapon used against ground targets -- a modern-day bazooka -- and it is not wire-guided.According to the Web site Globalsecurity.org, a loaded M136 AT4 anti-tank weapon has a 40-inch-long fiberglass-wrapped tube and weighs just 4 pounds. Its 84 millimeter shaped-charge missile can penetrate 14 inches of armor from a maximum of 985 feet. It is used once and discarded."
    summary = generate_summary(model, tokenizer, test_article)
    print("\nğŸ“ Generated Summary:")
    print(summary)


if __name__ == "__main__":
    main()
