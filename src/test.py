import os
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import sentencepiece as spm

# ======================================================
# ğŸ“ è·¨å¹³å°è·¯å¾„æ ¹ï¼šè„šæœ¬æ‰€åœ¨ç›®å½•
# ======================================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# ==============================
# æ¨¡å‹å®šä¹‰ (ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´)
# ==============================

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer("pe", pe)

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

class Attention(nn.Module):
    def __init__(self, d_model, head_dim):
        super().__init__()
        self.WQ = nn.Linear(d_model, head_dim, bias=False)
        self.WK = nn.Linear(d_model, head_dim, bias=False)
        self.V_up = nn.Linear(d_model, head_dim, bias=False)  # å…³é”®ï¼šæ”¹ä¸º V_up
        for m in [self.WQ, self.WK, self.V_up]:
            nn.init.xavier_uniform_(m.weight)

    def forward(self, Q, K, V, mask=None):
        Q = self.WQ(Q)
        K = self.WK(K)
        V_up = self.V_up(V)  # å…³é”®ï¼šæ”¹ä¸º V_up

        d_k = Q.size(-1)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, float("-inf"))

        attn = torch.softmax(scores, dim=-1)
        context = torch.matmul(attn, V_up)  # å…³é”®ï¼šä½¿ç”¨ V_up
        return context, attn

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0, "d_model å¿…é¡»èƒ½è¢« num_heads æ•´é™¤"
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads

        self.heads = nn.ModuleList([Attention(d_model, self.head_dim) for _ in range(num_heads)])
        self.V_down = nn.Linear(d_model, d_model, bias=False)  # å…³é”®ï¼šæ”¹ä¸º V_down
        nn.init.xavier_uniform_(self.V_down.weight)

    def forward(self, Q, K, V, mask=None):
        head_outputs = []
        for head in self.heads:
            context, _ = head(Q, K, V, mask)
            head_outputs.append(context)
        concatenated = torch.cat(head_outputs, dim=-1)
        output = self.V_down(concatenated)  # å…³é”®ï¼šä½¿ç”¨ V_down
        return output, None

class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.linear2(self.dropout(torch.relu(self.linear1(x))))

class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        self.ffn = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        attn_output, _ = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout(ffn_output))
        return x, None

class DecoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        self.cross_attn = MultiHeadAttention(d_model, num_heads)
        self.ffn = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_output, tgt_mask=None, memory_mask=None):
        self_attn_out, _ = self.self_attn(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(self_attn_out))
        cross_attn_out, attn_weights = self.cross_attn(x, enc_output, enc_output, memory_mask)
        x = self.norm2(x + self.dropout(cross_attn_out))
        ffn_out = self.ffn(x)
        x = self.norm3(x + self.dropout(ffn_out))
        return x, attn_weights

class Encoder(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model)
        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])
        self.norm = nn.LayerNorm(d_model)

    def forward(self, src, mask=None):
        x = self.embedding(src) * math.sqrt(self.embedding.embedding_dim)
        x = self.pos_encoding(x)
        for layer in self.layers:
            x, _ = layer(x, mask)
        x = self.norm(x)
        return x, None

class Decoder(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model)
        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])
        self.norm = nn.LayerNorm(d_model)

    def forward(self, tgt, enc_output, tgt_mask=None, memory_mask=None):
        x = self.embedding(tgt) * math.sqrt(self.embedding.embedding_dim)
        x = self.pos_encoding(x)
        for layer in self.layers:
            x, _ = layer(x, enc_output, tgt_mask, memory_mask)
        x = self.norm(x)
        return x, None

class TransformerModel(nn.Module):
    def __init__(self, vocab_size, d_model=512, num_heads=8, num_layers=6, d_ff=2048, dropout=0.1):
        super().__init__()
        self.encoder = Encoder(vocab_size, d_model, num_heads, d_ff, num_layers, dropout)
        self.decoder = Decoder(vocab_size, d_model, num_heads, d_ff, num_layers, dropout)
        self.output_layer = nn.Linear(d_model, vocab_size)

        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        enc_output, _ = self.encoder(src, src_mask)
        dec_output, _ = self.decoder(tgt, enc_output, tgt_mask, src_mask)
        output = self.output_layer(dec_output)
        return output

# ==============================
# ä¿®å¤åçš„æ‘˜è¦ç”Ÿæˆå™¨ç±»
# ==============================

class TextSummarizer:
    def __init__(self, model_path, tokenizer_path, max_len=512):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.max_len = max_len

        # è§„èŒƒåŒ–è·¯å¾„
        if not os.path.isabs(model_path):
            model_path = os.path.join(BASE_DIR, model_path)
        if not os.path.isabs(tokenizer_path):
            tokenizer_path = os.path.join(BASE_DIR, tokenizer_path)

        # åŠ è½½tokenizer
        if not os.path.exists(tokenizer_path):
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°tokenizeræ¨¡å‹: {tokenizer_path}")
        self.tokenizer = spm.SentencePieceProcessor(model_file=tokenizer_path)
        self.vocab_size = self.tokenizer.get_piece_size()

        print(f"âœ… TokenizeråŠ è½½æˆåŠŸï¼Œè¯è¡¨å¤§å°: {self.vocab_size}")
        print(f"ç‰¹æ®Šæ ‡è®° - PAD: {self.tokenizer.pad_id()}, BOS: {self.tokenizer.bos_id()}, EOS: {self.tokenizer.eos_id()}")

        # åˆå§‹åŒ–æ¨¡å‹ - å¿…é¡»ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼
        self.model = TransformerModel(
            vocab_size=self.vocab_size,
            d_model=128,      # æ ¹æ®ä½ çš„è®­ç»ƒä»£ç 
            num_heads=2,      # æ ¹æ®ä½ çš„è®­ç»ƒä»£ç 
            num_layers=2,     # æ ¹æ®ä½ çš„è®­ç»ƒä»£ç 
            d_ff=512          # æ ¹æ®ä½ çš„è®­ç»ƒä»£ç 
        ).to(self.device)

        # åŠ è½½æƒé‡
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶: {model_path}")
        
        print(f"ğŸ“‚ åŠ è½½æ¨¡å‹æƒé‡: {model_path}")
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # å…³é”®ï¼šåŠ è½½æƒé‡å¹¶å¤„ç†å¯èƒ½çš„é”®ä¸åŒ¹é…
        if isinstance(checkpoint, dict):
            if 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint
        else:
            state_dict = checkpoint
        
        # å°è¯•åŠ è½½æƒé‡
        try:
            self.model.load_state_dict(state_dict)
            print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ!")
        except Exception as e:
            print(f"âš ï¸ ç›´æ¥åŠ è½½å¤±è´¥ï¼Œå°è¯•å¤„ç†é”®ä¸åŒ¹é…: {e}")
            # å¤„ç†é”®ä¸åŒ¹é…
            model_dict = self.model.state_dict()
            
            # åˆ›å»ºæ˜ å°„å…³ç³»
            key_mapping = {}
            for key in state_dict.keys():
                if 'WV' in key:
                    new_key = key.replace('WV', 'V_up')
                elif 'WO' in key:
                    new_key = key.replace('WO', 'V_down')
                else:
                    new_key = key
                key_mapping[new_key] = key
            
            # åˆ›å»ºæ–°çš„state_dict
            new_state_dict = {}
            for model_key in model_dict.keys():
                if model_key in key_mapping:
                    # ç›´æ¥æ˜ å°„
                    new_state_dict[model_key] = state_dict[key_mapping[model_key]]
                elif model_key in state_dict:
                    # ç›´æ¥ä½¿ç”¨
                    new_state_dict[model_key] = state_dict[model_key]
                else:
                    # ä½¿ç”¨éšæœºåˆå§‹åŒ–
                    print(f"âš ï¸ æ‰¾ä¸åˆ°å‚æ•°: {model_key}ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
                    new_state_dict[model_key] = model_dict[model_key]
            
            # åŠ è½½å¤„ç†åçš„æƒé‡
            self.model.load_state_dict(new_state_dict)
            print("âœ… å¤„ç†åæƒé‡åŠ è½½æˆåŠŸ!")
        
        self.model.eval()
        print("âœ… æ–‡æœ¬æ‘˜è¦å™¨åˆå§‹åŒ–å®Œæˆ!")

    def generate_square_subsequent_mask(self, sz, device=None):
        """ç”Ÿæˆå› æœmask"""
        if device is None:
            device = self.device
        mask = torch.triu(torch.ones(sz, sz, device=device), diagonal=1)
        mask = mask.masked_fill(mask == 1, float('-inf'))
        return mask

    def summarize(self, text, max_length=80, temperature=1.0, top_k=50):
        """ç”Ÿæˆæ–‡æœ¬æ‘˜è¦"""
        # ç¼–ç è¾“å…¥æ–‡æœ¬
        src_ids = self.tokenizer.encode(text, out_type=int)
        src_ids = src_ids[:self.max_len]
        src_tensor = torch.tensor([src_ids], dtype=torch.long, device=self.device)

        # åˆå§‹åŒ–è§£ç å™¨è¾“å…¥
        tgt_ids = torch.tensor([[self.tokenizer.bos_id()]], dtype=torch.long, device=self.device)

        with torch.no_grad():
            for step in range(max_length):
                tgt_mask = self.generate_square_subsequent_mask(tgt_ids.size(1), self.device)
                output = self.model(src_tensor, tgt_ids, tgt_mask=tgt_mask)
                
                # è·å–ä¸‹ä¸€ä¸ªtokençš„logits
                next_token_logits = output[:, -1, :]
                
                # åº”ç”¨æ¸©åº¦è°ƒèŠ‚
                if temperature > 0 and temperature != 1.0:
                    next_token_logits = next_token_logits / temperature
                    
                    # Top-kç­›é€‰
                    if top_k > 0:
                        indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
                        next_token_logits[indices_to_remove] = -float('Inf')
                    
                    # é‡‡æ ·
                    probs = torch.softmax(next_token_logits, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)
                else:
                    # è´ªå©ªè§£ç 
                    next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)

                # æ·»åŠ åˆ°åºåˆ—
                tgt_ids = torch.cat([tgt_ids, next_token], dim=1)
                
                # é‡å¤æ£€æµ‹
                if step > 10:
                    recent_tokens = tgt_ids[0][-5:].tolist()
                    if len(set(recent_tokens)) == 1:
                        print("âš ï¸ æ£€æµ‹åˆ°é‡å¤ï¼Œæå‰ç»“æŸç”Ÿæˆ")
                        break
                
                # EOSåœæ­¢
                if next_token.item() == self.tokenizer.eos_id():
                    break

        # è§£ç ç”Ÿæˆç»“æœ
        generated_ids = tgt_ids[0].tolist()[1:]  # å»æ‰BOS
        if self.tokenizer.eos_id() in generated_ids:
            generated_ids = generated_ids[:generated_ids.index(self.tokenizer.eos_id())]
        
        return self.tokenizer.decode(generated_ids)

# ==============================
# ä¸»ç¨‹åº
# ==============================

def main():
    try:
        summarizer = TextSummarizer(
            model_path="best_transformer.pth",
            tokenizer_path="bpe_tokenizer.model"
        )

        print("\n" + "=" * 60)
        print("ğŸ“ æ–‡æœ¬æ‘˜è¦ç”Ÿæˆå™¨")
        print("=" * 60)
        print("è¾“å…¥ 'quit' é€€å‡ºç¨‹åº")
        print("=" * 60)

        while True:
            print("\n" + "-" * 40)
            text = input("\nè¯·è¾“å…¥è¦æ‘˜è¦çš„æ–‡æœ¬: ").strip()
            
            if text.lower() in {"quit", "é€€å‡º", "exit"}:
                print("ğŸ‘‹ å†è§!")
                break
                
            if not text:
                print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆæ–‡æœ¬")
                continue

            print("\nâ³ æ­£åœ¨ç”Ÿæˆæ‘˜è¦...")
            try:
                # å…ˆç”¨è´ªå©ªè§£ç æµ‹è¯•
                summary_greedy = summarizer.summarize(text, temperature=0, top_k=0)
                print(f"\nğŸ“„ åŸæ–‡: {text}")
                print(f"ğŸ“ è´ªå©ªè§£ç : {summary_greedy}")
                
                # å†ç”¨é‡‡æ ·ç”Ÿæˆ
                summary_sampled = summarizer.summarize(text, temperature=0.8, top_k=30)
                print(f"ğŸ“ é‡‡æ ·ç”Ÿæˆ: {summary_sampled}")
                
            except Exception as e:
                print(f"âŒ ç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {e}")

    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")

if __name__ == "__main__":
    main()